{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_samples(image_folder, annotation_folder, target_size):\n",
    "    training_samples = []\n",
    "\n",
    "    for image_file in os.listdir(image_folder):\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        annotation_path = os.path.join(annotation_folder, image_file.replace(\".jpg\", \".txt\"))\n",
    "        real_img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(real_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        with open(annotation_path, 'r') as annotation_file:\n",
    "            lines = annotation_file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                data = line.strip().split()\n",
    "                class_label = int(data[0])\n",
    "                x_center, y_center, width, height = map(float, data[1:5])\n",
    "\n",
    "\n",
    "                # Convert YOLO format to (x, y, w, h)\n",
    "                x = int((x_center - width / 2) * img.shape[1])\n",
    "                y = int((y_center - height / 2) * img.shape[0])\n",
    "                w = int(width * img.shape[1])\n",
    "                h = int(height * img.shape[0])\n",
    "\n",
    "                # Read the image and crop the region defined by the bounding box\n",
    "\n",
    "                # cropped_region = img[y:y+h, x:x+w]\n",
    "                cropped_region = img[max(y, 0):min(y+h, img.shape[0]), max(x, 0):min(x+w, img.shape[1])]\n",
    "\n",
    "                # Resize the cropped region to the target size\n",
    "                resized_cropped_region = cv2.resize(cropped_region, target_size)\n",
    "\n",
    "                # Normalize pixel values to [0, 1]\n",
    "                normalized_cropped_region = resized_cropped_region / 255.0\n",
    "\n",
    "                # Append the training sample along with its class label\n",
    "                training_samples.append((normalized_cropped_region, class_label))\n",
    "\n",
    "    return training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "image_folder = 'dataset/images'\n",
    "annotation_folder = 'dataset/labels'\n",
    "\n",
    "target_size = (224, 224)\n",
    "\n",
    "training_samples = create_training_samples(image_folder, annotation_folder, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_samples, val_samples = train_test_split(training_samples, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels from training samples\n",
    "X_train, y_train = zip(*train_samples)\n",
    "X_temp, y_temp = zip(*val_samples)\n",
    "X_val,X_test,y_val,y_test=train_test_split(X_temp, y_temp,test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test,y_test= np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base VGG19 model\n",
    "base_vgg_model = VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "# Freeze the layers of the InceptionV3 model\n",
    "base_vgg_model.trainable = False\n",
    "\n",
    "# Add custom layers for classification\n",
    "vgg_model = base_vgg_model.output\n",
    "vgg_model = GlobalAveragePooling2D()(vgg_model)\n",
    "vgg_model = Dense(1024, activation='relu')(vgg_model)\n",
    "predictions = Dense(1, activation='sigmoid')(vgg_model)  # Assuming you have 2 classes\n",
    "\n",
    "# Create the final model\n",
    "vgg = Model(inputs=base_vgg_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "vgg.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['Accuracy', 'Precision', 'Recall','AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "vgg.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
